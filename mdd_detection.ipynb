{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.backend import clear_session\n",
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "#read all file\n",
    "all_files_path=glob('../sd_copy/*.edf')\n",
    "print(len(all_files_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 86\n"
     ]
    }
   ],
   "source": [
    "healthy_file_path=[i for i in all_files_path if  'H' in i.split('\\\\')[1]]\n",
    "patient_file_path=[i for i in all_files_path if  'M' in i.split('\\\\')[1]]\n",
    "\n",
    "print(len(healthy_file_path),len(patient_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    # Select a specific channel\n",
    "    channel_to_keep = ['EEG Fp1-LE', 'EEG F3-LE', 'EEG C3-LE', 'EEG P3-LE', 'EEG O1-LE', 'EEG F7-LE', 'EEG T3-LE', 'EEG T5-LE', 'EEG Fz-LE', 'EEG Fp2-LE', 'EEG F4-LE', 'EEG C4-LE', 'EEG P4-LE', 'EEG O2-LE', 'EEG F8-LE', 'EEG T4-LE', 'EEG T6-LE', 'EEG Cz-LE', 'EEG Pz-LE', 'EEG A2-A1']  \n",
    "    # Replace with the name of the channel you want to keep\n",
    "    raw.pick_channels(channel_to_keep)\n",
    "    raw.set_eeg_reference()\n",
    "    raw.filter(l_freq=30,h_freq=100)#1-4=delta, 4-8=theta, 8-12=alpha, 12-30=beta, 30-100=gamma\n",
    "    epochs=mne.make_fixed_length_epochs(raw,duration=15,overlap=1)\n",
    "    epochs=epochs.get_data()\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(epochs.reshape(-1,epochs.shape[-1])).reshape(epochs.shape)\n",
    "    return data #trials,channel,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "control_epochs_array=[read_data(subject) for subject in healthy_file_path]\n",
    "patients_epochs_array=[read_data(subject) for subject in patient_file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_epochs_labels=[len(i)*[0] for i in control_epochs_array]\n",
    "patients_epochs_labels=[len(i)*[1] for i in patients_epochs_array]\n",
    "print(len(control_epochs_labels),len(patients_epochs_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=control_epochs_array+patients_epochs_array\n",
    "label_list=control_epochs_labels+patients_epochs_labels\n",
    "print(len(data_list),len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list=[[i]*len(j) for i, j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=np.vstack(data_list)\n",
    "label_array=np.hstack(label_list)\n",
    "group_array=np.hstack(groups_list)\n",
    "print(data_array.shape,label_array.shape,group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=np.moveaxis(data_array,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1,input_shape=(3840,20)))#1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#2\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#3\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#4\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#5\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#6\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#7\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#8\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#9\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(GlobalAveragePooling1D())#10\n",
    "    model.add(Dense(1,activation='sigmoid'))#11\n",
    "    \n",
    "    model.compile('adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf=GroupKFold(n_splits=10) #K-fold iterator with 10 fold\n",
    "accuracy=[]\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features,train_labels=data_array[train_index],label_array[train_index]\n",
    "    val_features,val_labels=data_array[val_index],label_array[val_index]\n",
    "    scaler=StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    model=cnnmodel()\n",
    "    model.fit(train_features,train_labels,epochs=50,batch_size=32,validation_data=(val_features,val_labels))\n",
    "    accuracy.append(model.evaluate(val_features,val_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gamma_15s_50epoch_32batch.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
